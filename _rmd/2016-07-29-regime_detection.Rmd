---
title: "Regime Switching Model"
author: matt_gregory
comments: yes
date: '2016-07-29'
modified: `r format(Sys.time(), '%Y-%m-%d')`
layout: post
excerpt: "Forecasting regime changes in market turbulence"
published: TRUE
status: processed
tags:
- Hidden Markov Models
- Regime detection
- forecast
- forecasting
- R
categories: Rstats
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  dev = "svg",
  include = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE
  )

```

I'm a sucker for statistical methods and Machine Learning particularly anything with a cool sounding name. When reading about [Crouching Tiger Hidden Markov Models](http://www.machinegurning.com/rstats/markov_chain_discrete/) in an earlier post I stumbled across a topic called [regime detection](http://www.r-bloggers.com/regime-detection/).  

In economics latent Markov models, are so called Regime switching models. Regime Detection comes in handy when you are trying to decide which strategy to deploy. For example there are periods (regimes) when Trend Following strategies, like an autoregressive integrated moving average model (ARIMA) or exponential smoothing state space models (ETS) [forecasting](https://www.otexts.org/fpp) work better and there are periods when other strategies might be better. This might be useful if you are forecasting an index or rate that typically follows a trend but occasionally becomes more volatile.  

For example some time series may be particularly well behaved except during the unpredictable economic downturns. The idea behind using the Regime Switching Models to identify market states is that market returns might have been drawn from two or more distinct distributions. Fortunately we do not have to fit regimes by hand, there is the `depmixS4 package` for Hidden Markov Models at CRAN that uses the expectation-maximization (EM) algorithm to fit Hidden Markov Models.

```{r echo=FALSE, warning=FALSE, message=FALSE}

#LIBRARY - check if packages are installed and load them
library(dplyr)
library(depmixS4)
library(zoo)
library(lubridate)
library(ggplot2)
library(reshape2)
#SETUP
rm(list = ls()) #  clear workspace

#SEED
set.seed(1337)

#INPUT
#  https://www.gov.uk/government/statistics/bis-prices-and-cost-indices
ukdata <- paste("data/2016-07-29-tpi", ".csv", sep = "")
ukdata <- read.csv(ukdata,
                  header = TRUE)  # in tidy dataframe format, one row per observation

#  removes zero tpi values and tpi forecasts into 2016
ukdata_noforecasts <- filter(ukdata, tpi > 0 & 
                               year(as.yearqtr(date, format = "%b-%y")) < 2015)  # we cahnge to 2015 here, due to 18 months to get fixed

df3 <- data.frame(thedate = as.yearqtr(ukdata_noforecasts$date, format = "%b-%y"),
                  tpi = ukdata_noforecasts$tpi,
                  stringsAsFactors = FALSE)
df4 <- mutate(df3, tpi_d1 = tpi - lag(tpi))  #  take first order difference to make stationary, default
# or see http://stackoverflow.com/questions/35169423/error-when-using-diff-function-inside-of-dplyr-mutate
df5 <- mutate(df3, tpi_d2 = c(NA, NA, diff(tpi, lag = 1, differences = 2)))

z3 <- zoo(x = df3$tpi, order.by = df3$thedate)
ts_uk_tpi <- as.ts(z3)
```

```{r 2016-07-29-plot_tpi}
plot(ts_uk_tpi, xlab = "Year", ylab = "UK BCIS TPI")  #  see Rmd for data processing

```

We use an economic indicator variable from the UK Building Cost Information Service (BCIS), as it provides an excellent demo of the type of variable that tends to have an upward trend until occasional market effects cause uncertainty and volatility. [Tender price index](https://www.gov.uk/government/statistics/bis-prices-and-cost-indices) (TPI) is used for many practical purposes in the construction industry, including establishing the level of individual tenders, adjustment for time, pricing, cost planning, and forecasting cost trends and general comparisons. Any index that responds to market conditions is suitable for this methodology.

Here it appears we have distinguishable states or regimes, the steady upward trend and the more volatile mountainous peaks followed by a slight trough. 

We use the package documentation from `vignette("depmixS4")` to get started. First we experiment on the original time series, then we will see the impact of looking at the TPI first order difference.

```{r}
#  following example from vignette
mod2 <- depmix(response = tpi ~ 1, data = df3, nstates = 2, trstart = runif(4))
summary(mod2)
#  mod contains the model specification not the model fit, hence
fm2 <- fit(mod2, emc = em.control(rand = TRUE))
print(fm2)

```

We can compare this to a trivial one state model which returns the mean and standard deviation of the modelled variable. The two state model is slightly better with smaller log-likelihood, Akaike information criterion (AIC) and Bayesian information criterion  (BIC) despite an increase in the degrees of freedom associated with the larger number of states modelled.

```{r}

mod1 <- depmix(tpi ~ 1, data = df3, nstates = 1)
summary(mod1)
fm1 <- fit(mod1, emc = em.control(rand = TRUE))
print(fm1)

```

# First order difference

As the time series is non-stationary, let's take the first order difference and lagged time series of the `tpi`, as this often helps when modelling time series. Again we see the peaks and troughs mirrored by areas of increased volatility.

```{r 2016-07-29-plot_tpi_dif1}
#  df4 is as a dataframe
plot(x = df4$thedate, y = df4$tpi_d1, xlab = "Year", ylab = "UK BCIS TPI difference", type = "l")

```

We fit a two state model which results in a reduction in the Log likelihood and both Information Criteria measures.

```{r}
#BEST GAUSSIAN HIDDEN MARKOV MODEL MODEL
#  Remove row containing NA
mod3 <- depmix(response = tpi_d1 ~ 1, data = df4[complete.cases(df4), ],
               nstates = 2, trstart = runif(4))
summary(mod3)
#  mod contains the model specification not the model fit, hence
fm3 <- fit(mod3, emc = em.control(rand = TRUE), verbose = FALSE)
print(fm3)

```

# Second order difference

From the second order difference it looks like we have two regime states which could be modelled by Gaussian distributions with different standard deviations.

```{r 2016-07-29-plot_tpi_dif2}
#  df5 is as a dataframe
plot(x = df5$thedate, y = df5$tpi_d2, xlab = "Year", ylab = "UK BCIS TPI second order difference", type = "l")

```

We fit a model, regime detection.

```{r}
#  Remove row containing NA
mod4 <- depmix(response = tpi_d2 ~ 1, data = df5[complete.cases(df5), ],
               nstates = 2, trstart = runif(4))
summary(mod4)
#  mod contains the model specification not the model fit, hence
fm4 <- fit(mod4, emc = em.control(rand = TRUE), verbose = FALSE)
print(fm4)

```

# Which state are we in?
So our hidden Markov model explains more of the variation when fitted to the first order difference of the Tender Price Index using a two state model.

```{r}

(gauss_fit <- summary(fm3, which = "response"))
#  Where state 1 is the lower volatility and more upward trendy state
#  Where state 2 is the higher volatility, less predictable state
#  If in state 2 our time series methods are less likely to be useful for forecasting

st1_mean <- gauss_fit[1, 1]
st1_sd <- gauss_fit[1, 2]
st2_mean <- gauss_fit[2, 1]
st2_sd <- gauss_fit[2, 2]

```

Thus if we plot our TPI first order difference and add these distributions, we can elucidate when the TPI is in one or the other state (steady or volatile). The mean for both is above zero due to the non-stationary nature of the TPI as it wanders ever upwards through time, like other inflation indices. The standard deviation for the second volatile state is greater, reflecting the uncertainity and difficulting in forecasting TPI while in this state.

We can plot the first order difference of TPI and identify the different regimes using the Gaussian 95% confidence intervals (mean +/- 1.96*sd). If the TPI first order difference lies outside the 95%.

```{r 2016-07-29-plot_hmm}
#INPUT
ci <- c(1.96, 2.58) # 1.96 = 95%, 2.58 = 99%
line_type <- c("dashed", "solid")

#OUTPUT
plot(x = df4$thedate, y = df4$tpi_d1, xlab = "Year",
     ylab = "UK BCIS TPI first order difference",
     type = "l", ylim = c(-18, 18))
abline(h = st1_mean + ci*st1_sd, col = "green", lty = line_type)
abline(h = st1_mean - ci*st1_sd, col = "green", lty = line_type)
abline(h = st2_mean + ci*st2_sd, col = "red", lty = line_type)
abline(h = st2_mean - ci*st2_sd, col = "red", lty = line_type)

```

This is quite useful for identifying when TPI is in its different states, however it is of post hoc interest, as we can only look at it after the fact. However, as the volatile years putatively associated with state 2 tend to persist for several quarters, if we enter this state we can predict that our standard time series methods will not be useful for several quarters until the TPI first order difference generative model transitions back to state 1 with probability 0.522 as described in the transition matrix.

$$\mathbf{X} = \left[\begin{array}
{rrr}
0.581 & 0.419 \\
0.478 & 0.522 \\
\end{array}\right]
$$

The problem is there is some overlap between states. How can we tell which state the TPI is in?

## Which state when?
First we need to build a dataframe for a ggplot2 class object and not use the zoo class `yearqrt` for our dates. We place the Quarterly style with full dates and assume Quarters occur on the first day of the month of January, April, July, October.

```{r}
# http://blog.revolutionanalytics.com/2014/03/r-and-hidden-markov-models.html
# Build a data frame for ggplot
dfgg <- data.frame(df4)
dfgg$thedate <- as.Date.yearqtr(dfgg$thedate, format = "%Y Q%q")
str(dfgg)
```

This looks good and allows us to plot. The graph shows what looks like a more or less stationary process punctuated by a few spikes of extreme volatility. Have a guess as to when the most extreme spike occurs? 

```{r 2016-07-29-plot_tpi_black}

mycolours <- "black"

p1 <- ggplot( dfgg, aes(thedate) ) +
  geom_line( aes( y = tpi_d1 ), colour = mycolours) +
  labs( title = "UK BCIS TPI first order difference") +
  ylab("Change in TPI") + xlab("Year") + 
  theme(legend.position = "bottom", legend.direction = "horizontal",
        legend.title = element_blank()) +
  theme_bw()

p1
```

```{r 2016-07-29-plot_tpi_blue}
# Economist theme
library(ggthemes)
#library(extrafont)

#hybrid, couldn't get economist font, requires effort
p2 <- p1 + theme_economist() + scale_colour_economist() +
  theme(axis.line.x = element_line(size = .5, colour = "black"),
        legend.position = "bottom", legend.direction = "horizontal",
        legend.title = element_blank())
p2

#When was the most extreme spike?
# min(dfgg$tpi_d1, na.rm = T)
# dfgg[dfgg$tpi_d1 == -17, ]  #  2009 Q1

#upward peak?
#max(dfgg$tpi_d1, na.rm = T)
#dfgg[dfgg$tpi_d1 == 15, ]  #  2004 Q4 and 2012 Q4

```

Let's construct and fit a regime switching model and confirm while we are at it, that the 2 state model is superior. It is, try adjusting the `nstates` argument to confirm, which gives the lowest log-likelihood and AIC and BIC?

```{r}

# Construct and fit a regime switching model
mod5 <- depmix(tpi_d1 ~ 1, family = gaussian(), nstates = 2,  #  change this and see, 2 is best for lowest AIC & BIC
               data = dfgg[complete.cases(dfgg), ])
set.seed(1337)
fm5 <- fit(mod5, verbose = FALSE)
#
summary(fm5)
print(fm5)

```

Now we have an inference task where we know the mean and standard deviation of the two different states, thus we can infer the proability that an observation belongs to a given state, either state 1 (calm) or state 2 (volatile).

```{r}

# Classification (inference task)
probs <- posterior(fm5)             # Compute probability of being in each state
head(probs)
rowSums(head(probs)[, 2:3])          # Check that probabilities sum to 1

pCalm <- probs[, 2]                  # Pick out the "Bear" or low volatility state, nod to economics
dfgg$pCalm <- c(0.99, pCalm)  # remember we removed the NA from earlier, we make Bear state with high probs
# Put pCalm in the data frame for plotting

```

Now we have the probabilities of each state our Bear (calm, state 1) and our volatile state (state 2).

```{r 2016-07-29-plot_tpi_hmm_prob}

# reshape the data in a form convenient for ggplot
df <- melt(dfgg[, c("thedate", "tpi", "tpi_d1", "pCalm")],
           id = "thedate",
           measure = c("tpi", "tpi_d1","pCalm"))
#head(df)

# Plot the tpi time series along withe the time series of probabilities
#qplot(thedate, value, data = df, geom = "line",
#      main = "Quarterly change in TPI and 'Calm' state probabilities",
#      ylab = "") + 
#  facet_grid(variable ~ ., scales = "free_y")

p3 <- ggplot(df, aes(thedate, value, col = variable, group = 1)) +
  geom_line() +
 facet_grid(variable~., scale = 'free_y') +
  scale_color_discrete(breaks = c('tpi', 'tpi_d1','pCalm')) +
  theme_bw()
p3

```

This tells us the current volatility of the TPI and thus will determine the utility and precision of our forecasts that rely on standard timeseries ARIMA methods. Given the TPI is currently in a volatile state probably, we should be cautious when using our standard forecasting strategies. This is particularly poignant given the market volatility associated with the EU referendum.

The states prove to be quite sticky and unlikely to change as indicated by the transition matrix:

$$\mathbf{X} = \left[\begin{array}
{rrr}
0.98 & 0.02 \\
0.03 & 0.97 \\
\end{array}\right]
$$

## Forecasting

Perhaps we can use markovchain package to run simulations and determine most probable scenario to assist forecast.

```{r warning=FALSE, message=FALSE}

library(markovchain)

#define the Markov chain
statesNames = c("Calm", "Volatile")
mc_tpi <- new("markovchain", states = statesNames,
           transitionMatrix = matrix(c(0.98, 0.02, 0.03, 0.97), 
                                     nrow = 2, byrow = TRUE,
                                     dimnames = list(statesNames, statesNames)
                 ))
#show the sequence
outs2 <- rmarkovchain(n = 8, object = mc_tpi)

```

If we look at the tail end of the first order difference of the TPI:

```{r}
tail(df4$tpi_d1)

```

We observe TPI difference of less than the 99% CI for State 2. Therefore we assume that the TPI difference is in State 1 to initiate our simulation, although it could be in state 2 given the previous large differences of 15 & 7.

Thus we run the simulation.

```{r}

outs <- markovchainSequence(n = 8, markovchain = mc_tpi, t0 = "Volatile")
outs


```

We could run this 10,000 times and build up a probability distribution of likely states for future TPI to assist with time series forecasting using traditional time series methods that rely on historic data to predict the future.

# Conclusion

Hybrid models can be developed which can add confidence to using traditional time series methods such as ARIMA and ETS, whereby we expect the future to behave in a similar fashion to the past (especially less volatile periods in an indices history).

> Prediction is very difficult, especially about the future.

*- Niels Bohr*


```{r}
sessionInfo()
```

