---
title: "Logistic regression for student performance prediction"
author: matt_gregory
comments: yes
date: '2016-03-15'
modified: `r format(Sys.time(), '%Y-%m-%d')`
layout: post
excerpt: "Predicting student end of year performance using logistic regression"
published: yes
status: processed
tags:
- R
- machine learning
- education
categories: Rstats
---

{% include _toc.html %}

```{r setup,include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  dev = "svg"
  )
```

## Introduction

Classification problems occur often, perhaps even more so than regression problems. Consider the [Cortez student maths attainment data](https://archive.ics.uci.edu/ml/datasets/Student+Performance) discussed in previous [posts](http://www.machinegurning.com/rstats/student-performance/). The response variable, final grade of the year (range 0-20), `G3` can be classified into a binary pass or fail variable called `final`, based on a threshold mark. We used a decision tree approach to model this data before which provided 95% accuracy and had the benefit of interpretability. We will now model this using logistic regression so we can attach probabilities to our student pass or fail predictions. 

```{r,warning=FALSE,message=FALSE}


library(dplyr)
library(ROCR)
library(caret)
library(e1071)
library(boot)

#INPUT
mydata <- "data/2016-03-15-logreg_math.csv" 
mydata <- read.table(mydata, sep = ";",
                     header = TRUE)

```

## Make the final grade binary (pass and fail)
`G3` is pretty normally distributed, despite the dodgy tail. To simplify matters converted `G3` marks below 10 as a fail, above or equal to 10 as a pass. Often a school is judged by whether students meet a critcal boundary, in the UK it is a C grade at GCSE for example. Rather than modelling this response Y directly, logistic regression models the probability that Y belongs to a particular category.

```{r,echo=TRUE}

mydata$final <- NULL
mydata$final <- factor(ifelse(mydata$G3 >= 10, 1, 0),
                       labels = c("fail", "pass"))
data_interest <- mydata

```

From our learnings of the decision tree we can include the variables that were shown to be important predictors in this multiple logistic regression.

## Objective  
- Using the training data estimate the regression coefficients using maximum likelihood.  
- Use these coefficients to predict the test data and compare with reality.
- Evaluate the binary classifier with receiver operating characteristic curve (ROC).
- Evaluate the logistic regression performance with the resampling method cross-validation

## Training and test datasets.
We need to split the data so we can build the model and then test it, to see if it generalises well. The data arrived in a random order.

```{r,echo=TRUE}

data_train <- data_interest[1:350, ]
data_test <- data_interest[351:395, ]

```

Now we need to train the model using the data. From our decision tree we know that the prior attainment data variables `G1` and `G2` are important as are the `Fjob` and `reason` variables. We fit a logistic regression model in order to predict `final` using the variables mentioned in the previous sentence.

```{r}

m1 <- glm(final ~ G1 + G2 + Fjob + reason, data = data_train, family = binomial)
summary(m1)
  
```
The model does appear to suffer from overdispersion. The p-values associated with `reason` are all non-significant. Following Crawley's recommendation we attempt model simplification by removing this term from the model after changing the model family argument to `family = quasibinomial`. 

```{r}

m1 <- glm(final ~ G1 + G2 + Fjob + reason, data = data_train, family = quasibinomial)

```
We use the more conservative "F-test" to compare models due to the quasibinomial error distribution, after Crawley.

```{r}

m2 <- update(m1, ~. - reason)  #  the model is identical except removal of reason variable
anova(m1, m2, test = "F") 

```

No difference in explanatory power between the models. There is no evidence that `reason` is associated with a students pass or fail in their end of year maths exam. We continue model simplification after using `summary()` (not shown).

```{r}

m3 <- update(m2, ~. - G1)
anova(m2, m3, test = "F")

```

We don't need the earlier `G1` exam result as we have `G2` in the model already. What happens if we remove `Fjob`?

```{r}
m4 <- update(m3, ~. - Fjob)
anova(m3, m4, test = "F")

```

We lose explanatory power, we need to keep `Fjob` in the model. This gives us our minimal adequate model. `Fjob` is a useful predictor but perhaps we could reduce the number of levels by recoding the variable as only some of the jobs seem useful as predictors.

## Contrasts

For a better understanding of how R dealt with the categorical variables, we can use the `contrasts()` function. This function will show us how the variables have been dummyfied by R and how to interpret them in a model. Note how the default in R is to use alphabetical order.

```{r}
contrasts(data_train$final)  #  fail as zero, pass as one; logical
contrasts(data_train$Fjob)
```

## Model interpretation

```{r}

summary(m3)

```

The smallest p-value here is assocaited with `G2`. The positive coefficient for this predictor suggests that an increase in `G2` is associated increase in the probability of `final = pass`. To be precise a one-unit increase in `G2` is associated with an increase in the log odds of `pass` by `r coef(m3)[2]`.

```{r}

glm.probs <- predict(m3, newdata = data_test, type = "response")  # predicted probabilities
glm.pred <- rep("fail", 45)  #  convert into pass or fail
glm.pred[glm.probs > 0.5] = "pass"  #  index

confusionMatrix(table(glm.pred, data_test$final), positive = "pass")  # from the caret package, also need e1071 package

```

```{r,include=FALSE}

# Note moved this to a chunk as complicated in-line code is difficult to debug.

ref1 <- round(100 - (mean(glm.pred == data_test$final)*100), 3)

```

The first command predicts the probability of the test students' characteristics resulting in a `pass` based on the `glm()` built using the training data. The second and third command creates a vector of `r nrow(data_test)` `fails` with those probabilities greater than 50% being converted into `pass`. The predicted passes and failures are compared with the real ones in a table with a test error of `r ref1`%.

## Model performance
As a last step, we are going to plot the ROC curve and calculate the AUC (area under the curve) which are typical performance measurements for a binary classifier.
 The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.
 
```{r 2016-03-15_plot_prf}

pr <- prediction(glm.probs, data_test$final)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

```


## Conclusion
The 0.95 accuracy on the test set is quite a good result and an AUC of `r auc`. However, keep in mind that this result is somewhat dependent on the manual split of the data that I made earlier, therefore if you wish for a more precise score, you would be better off running some kind of cross validation such as k-fold cross validation. The logistic regression also provides coefficients allowing a quantitative understanding of the association between a variable and the odss of success which can be useful.

## Leave-one-out cross-validation for Generalized Linear Models
As mentioned above let's conduct a cross validation using the `cv.glm()` function from the [boot](http://www.inside-r.org/r-doc/boot/cv.glm) package.This function calculates the estimated K-fold cross-validation prediction error for generalized linear models. We produce our model `glm.fit` based on our earlier learnings. We follow guidance of the Chapter 5.3.2 cross-validation lab session in James et al., 2014.

```{r, eval = TRUE}

set.seed(1337)
glm.fit <- glm(final ~ G2 + Fjob, family = quasibinomial, data = data_interest)
cv.err <- cv.glm(data = data_interest, glmfit = glm.fit)
cv.err$delta

```

```{r,include=FALSE}

ref1 <- round(cv.err$delta[1], digits = 3)

```

The `cv.glm()` function produces a list with several components. The two numbers in the `delta` vector contain the cross-validation results. Our cross-validation estimate for the test error is approximately `r ref1`.

## k-fold cross-validation
The `cv.glm()` function can also be used to implement k-fold cross-validation. Below we use k = 10, a common choice for k, on our data.

```{r}

set.seed(1337)
cv.err.10 <- cv.glm(data = data_interest, glmfit = glm.fit, K = 10)
cv.err.10$delta

```
On this data set, using this model, the two estimates are very close for K = 1 and K = 10. The error estimates are small, suggesting the model may perform OK if applied to predict future student `final` pass or fail.

## References
* Cortez and Silva (2008). Using data mining to predict secondary school performance.
* Crawley (2004). Statistics an introduction using R.
* James et al., (2014). An introduction to statistical learning with applications in R. Springer.
* http://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/
* https://archive.ics.uci.edu/ml/datasets/Student+Performance

```{r}

sessionInfo()

```

