---
title: "Nearest neighbour methods"
author: matt_upson
comments: yes
date: '2016-05-27'
modified: `r format(Sys.time(), '%Y-%m-%d')`
layout: post
excerpt: "A simple non-linear classifier using nearest-neighbour averaging"
published: true
status: processed
tags:
- statistics
- data science
- R
- machine learning
- nearest neighbours
categories: Rstats
---

{% include _toc.html %}

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  dev = "svg",
  include = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE
  )
```

In my [last post](../least_squares_and_nearest_neighbours/), I started working through some examples given by Hastie et al in Elements of Statistical learning.
I looked at using a linear model for classification across a randomly generated training set.
In this post I'll use nearest neighbour methods to create a non-linear decision boundary over the same data.

## Nearest neighbour algorithm

There are much more learned folk than I who give good explanations of the maths behind nearest neighbours, so I won't spend too long on the theory.
Hastie et al define the nearest neighbour approach as:

$$
\hat{Y}(x)=\frac{1}{k}\sum_{x_i\in N_k(x)}y_i
$$

The $k$ refers to the number of groups that we are interested in, and is user defined.
Our prediction $\hat{Y}$ (which is derived from $x$) is equal to the mean of $N_k$, where $N_k$ consists of the $k$ nearest training examples closest to $x$.

How do we define this closeness? Hastie et al simply use Euclidean distance:

$$
N_k(x) = \sqrt{\sum_{i=1}^n(x_i - x)^2\\}
$$

So, simply put, all we need to do is look at the neighbours of a particular training example, and take an average of them, to create our prediction of the score of a given point.

## R Walkthrough

As ever, the code to produce this post is available on github, [here](https://github.com/machinegurning/machinegurning.github.io/tree/master/_rmd).  

Using the data I generated in my previous [post](../least_squares_and_nearest_neighbours/) I'll walk through the process of producing a nearest neighbour prediction.

```{r , include=FALSE}

# Load packages and data generated in the last post

library(purrr)
library(tibble)
library(magrittr)
library(dplyr)
library(ggplot2)
library(tidyr)

## Load data and rename some of the variables (should have named them better in
## the first place!)

D <- "data/2016-05-08-dummy_data.Rds" %>%
  readRDS %>%
  transmute(
    x1 = X,
    x2 = Y,
    y = group_bit
  )

```

Just to recap: this is a dataset with 300 training examples, two features ($x_1$ and $x_2$), and a binary coded response variable ($X\in\mathbb{R}^{300 \times 2}$, $y\in\{0,1\}$):

```{r, echo=FALSE}

X <- D[,c("x1","x2")]

X

y <- D$y

y[1:10]

```

### Calculating Euclidean distance

The first thing I need to do is calculate the Euclidean distance from every training example to every other training example - i.e. create a distance matrix. Fortunately R does this very simply with the `dist()` command.
This returns a $m \times m$ dimensional matrix

```{r}
## Note that this creates a dist object, and needs to be coerced into a vector.

dist_matrix <- X %>%
  dist %>%
  as.matrix

## For brevity, I won't print it. Instead check the dimensions

dist_matrix %>% dim

```

I'm interested in the 15 nearest neighbour average, like Hastie et al., so I just need need to extract the 15 shortest distances from each of these columns. 
It helps at this point to break the matrix into a list using `split()`, with a vector element where each column was. 
This will allow me to use `purrr::map()` which has an easier syntax than other loop handlers like `apply()` and its cousins.

```{r}

## Use split to convert the matrix into a list

dist_matrix_split <-  dist_matrix %>% 
  split(
    f = rep(1:ncol(dist_matrix), each = nrow(dist_matrix))
  )

## Check that we have a list of length m

dist_matrix_split %>% class
dist_matrix_split %>% length

## What about a single element of the list?

dist_matrix_split %>% extract2(1) %>% class
dist_matrix_split %>% extract2(1) %>% length

```

Now I need a small helper function to return the closest $k$ points, so that I can take an average.
For this I use `order()`

```{r}

return_k <- function(x,k) {
  
  order(x)[1:k] %>%
    return
}

```

This should return a vector element in the list containing the index of $D$ which corresponds to the $k$ closest training examples.

```{r}

ranks <- dist_matrix_split %>%
  map(return_k, k = 15)

ranks[1:2]

```

So far so good, the function returns us 15 indices with which we can subset $y$ to get our 15 nearest neighbour majority vote.
The values of $y$ are then averaged...

```{r}

# Note I coerce to character first, then to integer, otherwise our integers are
# not zero indexed.

y_hat <- ranks %>%
  map(function(x) y[x]) %>%
  map_dbl(function(x) x %>% as.character %>% as.integer %>% mean)

y_hat[1:10]

```

...and converted into a binary classification, such that $G\in\{0,1\}$: where $\hat{Y}>0.5$, $G=1$, otherwise $G=0$.

```{r}

G <- ifelse(y_hat >= 0.5, 1, 0)  

G[1:10]

```

### Intuition

Before looking at the predictions, now is a good point for a quick recap on what the model is actually doing.

For the training examples $(10, 47, 120)$ I have run the code above, and plotted out the 15 nearest neighbours whose $y$ is averaged to get out prediction $G$.

For the right hand point you can see that for all of the 15 nearest neighbours $y=1$, hence for our binary prediction $G=1$.
The opposite can be said for the left hand: again there is a unanimous vote, and so $G=0$.
For the middle point, most of the time $y=1$, hence although there is not unanimity, our prediction for this point would be $G=1$.

You can image that from this plot: whilst varying $k$ would have little effect on the points that are firmly within the respective classes, points close to the decision boundary are likely to be affected by small changes in $k$.
Set $k$ too low, and we invite *bias*, set $k$ too high, and we are likely to increase *variance*.
I'll come back to this.

```{r 2016-05-27-intuition, echo=FALSE, include=TRUE}

example_points <- c(10, 47, 120)

averaged_points <- dist_matrix_split[example_points] %>%
  map(return_k, k = 15) %>%
  map(as.data.frame) %>%
  rbind_all %>%
  extract2(1)


# Combine X, y and prediction made using nearest neighbours (G)

D2 <- cbind(D, G) %>%
  mutate(
    actual = factor(y),
    prediction = factor(G)
    )

D2 %>%
    ggplot +
    aes(
      x = x1,
      y = x2,
      colour = actual
    ) +
    geom_point(
      size = 3
    ) +
    coord_cartesian(
      xlim = c(-2.2,2.8),
      ylim = c(-3,3.5)
    ) +
    xlab("X") +
    ylab("Y") +
  geom_point(
    data = X[averaged_points,],
    size = 1,
    colour = "black"
  ) +
  geom_point(
    data = X[example_points,],
    size = 10,
    shape = 3,
    colour = "red"
  )

```

### Predictions

So how do the predictions made by nearest neighbours ($k=15$) match up with the actual values of $y$ in this training set?

```{r}
table(y,G)
```

In general: pretty good, and marginally better than the linear classifier I used in the previous [post]().
In just $`r round(10/300 * 100)`$% of cases does our classifier get it wrong.

### Decision boundary

For this next plot, I use the `class::knn()` function to replace the long-winded code I produced earlier.
This function allows us to train our classifier on a training set, and then apply it to a test set, all in one simple function.

In this case I produce a test set which is just a grid of points. By applying the model to this data, I can produce a decision boundary which can be plotted.

```{r 2016-05-27-decision_boundary}

## Generate a grid of points

test_grid = seq(-3.5, 3.5, length = 800)

X_test <- data.frame(
  x1 = rep(test_grid, times = length(test_grid)), 
  x2 = rep(test_grid, each = length(test_grid))
)

# Run knn on our training set X and output predictions into a dataframe alongside X_test.

knn_pred <- class::knn(
  train = X,
  test = X_test,
  cl = y,
  k = 15
) %>%
  data.frame(
    X_test, 
    pred = .
  )

# Now plot,using geom_contour to draw the decision boundary.

knn_pred %>%
  ggplot +
  aes(
    x = x1,
    y = x2,
    colour = y
  ) +
  geom_point(
    data = D2,
    aes(
      colour = actual,
      shape = prediction
    ),
    size = 3
  ) +
  geom_contour(
    aes(
      z = as.integer(pred)
    ),
    size = 0.4,
    colour = "black",
    bins = 1
  ) +
  coord_cartesian(
    xlim = c(-2.2,2.8),
    ylim = c(-3,3.5)
  ) +
  xlab("X") +
  ylab("Y")

```

### Varying k

I mentioned before the impact that varying $k$ might have.
Here I have run `knn()` on the same data but for multiple values of $k$.
For $k=1$ we get a perfect fit with multiple polygons separating all points in each class perfectly.
As $k$ increases, we see that the more peripheral polygons start to break down, until at $k=15$ there is largely a singular decision boundary which weaves its way between the two classes.
At $k=99$, this decision boundary is much more linear.

```{r 2016-05-27-varying-k, fig.height=30, echo=FALSE}

k <- list(1,3,9,15,21,99)
k1 <- k %>%
  map(
    function(x) {
      class::knn(
        train = X, 
        test = X_test, 
        cl = y, 
        k = x
      ) %>%
        data.frame(G = .) %>%
        mutate(
          x1 = X_test$x1,
          x2 = X_test$x2,
          k = x
          )
    }
  ) %>%
  bind_rows


label_function <- function(string) {
  paste("k =",string)
}


k1 %>%
  ggplot +
  aes(
    x = x1,
    y = x2,
    colour = y
  ) +
  geom_point(
    data = D2,
    aes(
      colour = actual,
      shape = prediction
    ),
    size = 3
  ) +
  geom_contour(
    aes(
      z = as.integer(G),
      group = k
    ),
    size = 0.4,
    colour = "black",
    bins = 1
  )+
  facet_wrap(
    ~k,
    ncol = 1,
    labeller = labeller(k = label_function)
    ) +
  coord_cartesian(
    xlim = c(-2.2,2.8),
    ylim = c(-3,3.5)
  ) +
  xlab("X") +
  ylab("Y")

```

In my next post I will address this problem of setting $k$ again, and try to quantify when the model is suffering from variance or bias.

```{r,include=TRUE}
sessionInfo()
```
